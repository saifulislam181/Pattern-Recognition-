# -*- coding: utf-8 -*-
"""Decessin_Tree_Try.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zNM3pkwZSjycbCSHqvMR2qCGMH3WDCE7
"""

import numpy as np
import math
import random
import pandas as pd
import sys
from sklearn import preprocessing
from sklearn.metrics import f1_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split

data = pd.read_csv("dataset.csv")

x = data.loc[:, data.columns != "Class"]  
y = data.iloc[:, ].Class

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()

data['Class'].unique()

le = preprocessing.LabelEncoder()
for col in data.columns:
    data[col] = le.fit_transform(data[col])

x = data.loc[:, data.columns != "Class"]  
y = data.iloc[:, ].Class

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.10, stratify=y)

class Node:
    def __init__(self, attribute=None, attribute_values=None, child_nodes=None, decision=None):
        self.attribute = attribute
        self.attribute_values = attribute_values
        self.child_nodes = child_nodes
        self.decision = decision

class DecisionTree:
    root = None
    cur = None
    @staticmethod
    def plurality_values(data):
        labels = data[:, data.shape[1] - 1]  
        if list(labels).count("negative") > list(labels).count("positive"):
            return "negative"
        else:
            return "positive"

    @staticmethod
    def all_zero(data):
        labels = data[:, data.shape[1] - 1]  
        if list(labels).count("negative") == len(list(labels)):
            return True
        return False

    @staticmethod
    def all_one(data):
        labels = data[:, data.shape[1] - 1] 
        if list(labels).count("positive") == len(list(labels)):
            return True
        return False

    @staticmethod
    def entropy(q):
        try:
            return -q * math.log(q, 2) - (1 - q) * math.log(1 - q, 2)
        except:
            return 0


    def importance(self, data, attributes):
        labels = data[:, data.shape[1] - 1]  
        positives = list(labels).count("positive")
        negatives = list(labels).count("negative")
        total_value=(positives + negatives)

        entropy_parent = self.entropy(positives / total_value)

        # print(entropy_parent)
        ind_dict = {}

        res = []
        for item in attributes:
            features = list(np.unique(data[:, item]))
            total_child_entropy = 0

            for i in features:

                indivisual_features = (set(np.where(data[:, item] == i)[0]))
                indivisual_class = np.where(labels == "positive")[0]

                posit = len(indivisual_features.intersection(indivisual_class))

                indivisual_class = np.where(labels == "negative")[0]
                neg = len(indivisual_features.intersection(indivisual_class))

                child_tot_val = (posit + neg)

                entropy_child = self.entropy(posit /child_tot_val )
                
                entrop=(child_tot_val/ total_value)

                total_child_entropy += (entrop * entropy_child)


            ind_dict[item] = entropy_parent - total_child_entropy

        for k, v in (ind_dict.items()):

            if v == max(list(ind_dict.values())):
                return k

    def train(self, data, attributes, parent_data):
        data = np.array(data)
        parent_data = np.array(parent_data)
        attributes = list(attributes)

        if data.shape[0] == 0:  # if x is empty
            return Node(decision=self.plurality_values(parent_data))

        elif self.all_zero(data):
            return Node(decision=0)

        elif self.all_one(data):
            return Node(decision=1)

        elif len(attributes) == 0:
            return Node(decision=self.plurality_values(data))

        else:
            a = self.importance(data, attributes)

            tree = Node(attribute=a, attribute_values=np.unique(data[:, a]), child_nodes=[])

            attributes.remove(a)

            for vk in np.unique(data[:, a]):
                new_data = data[data[:, a] == vk, :]
                subtree = self.train(new_data, attributes, data)
                tree.child_nodes.append(subtree)

            return tree

    def fit(self, data):

        self.root = self.train(data, list(range(data.shape[1] - 1)), np.array([]))

    def predict(self, data):
        predictions = []
        for i in range(data.shape[0]):
            current_node = self.root
            while True:
                if current_node.decision is None:
                    current_attribute = current_node.attribute

                    current_attribute_value = data.iloc[i, current_attribute]
                    if current_attribute_value not in current_node.attribute_values:
                        predictions.append(random.randint(0, 1))
                        break
                    idx = list(current_node.attribute_values).index(current_attribute_value)

                    current_node = current_node.child_nodes[idx]
                else:
                    predictions.append(current_node.decision)
                    break

        return predictions

data = pd.read_csv("dataset.csv")

x_train = data.sample(frac=0.80)

x_test = data.merge(X_train, how='left', indicator=True)
x_test = x_test[x_test['_merge'] == 'left_only']
x_test = x_test.iloc[:, :-1]
y = le.fit_transform(np.array(x_test["Class"]))

dt = DecisionTree()
dt.fit(X_train)
y_pred = np.array(dt.predict(X_test))

print("Accuracy Score : ",accuracy_score(y, y_pred) *100,"%")
print("Precision score : ",precision_score(y, y_pred) *100,"%")
print("Recall Score : ",recall_score(y, y_pred) *100,"%")
print("F1 Score : ",f1_score(y, y_pred) *100,"%")